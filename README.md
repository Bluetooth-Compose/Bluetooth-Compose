## Hi there ðŸ‘‹

*Bluetooth-Compose is all about the convergence of Bluetooth Devices with Augmented Reality, with or without the Augmented Reality! Because just like SchrÃ¶dinger's cat, you don't know what's alive until you have a look! And because what's good for the goose is good for the gander, then take a gander at this!*

### A Sweet Introduction

Before we introduce Bluetooth-Compose, let's get a little excited by the convergence of Augmented Reality (AR) with Bluetooth Devices! 

- You will immediately see what Bluetooth-Compose does, and why it is important ~ not just to you, but to reducing the carbon-footprint of every-day devices around you!

So as an introduction, you may have already seen IKEA's wonderful Augmented Reality app where you can choose a piece of furniture and just point your phone at that empty space in your room where you would like to see see it, ... say a chair or sofa. 

- *Voila* - you see that sofa or chair augmented into the empty space in your room, and yet what is ironic about this is that IKEA's AR space is **only** useful *when an item <u>doesn't</u> exist* in your room!

Well Bluetooth-Compose turns that concept on it's head by being useful when something DOES exist in your room. 

- We call this real things in real space. This is how it's always been.

Yet the difference here is that for real Bluetooth devices in real space, their reality can be augmented by Bluetooth-Compose so that your bluetooth-device's virtual instrumentation can be rendered with it ... it's as though Bluetooth-Compose attaches a virtual computer monitor to every real bluetooth device that you have in the real-world around you, ... and that virtual monitor is capable of displaying what the bluetooth-device is doing, in real time! *And giving you the capacity to interact with it!*

- If you want to save the world, this is a good way to start doing it because every device's carbon-footprint can be reduced by shifting it's instrumentation facilities into the Augmented Reality space!
- If you want to save yourself by decluttering your real space from instrumentation, just shift the instrumentation from real space into augmented space, so it only exists when you look at it.
- If you want to value-add the space around you, let the device empower you where it is at, instead of where the clutter of your browser obscures it.

### So let's get down to basics.

Those familiar with docker-compose.yml files will very likely already assume that a ```bluetooth-compose.yml``` file does something similarly very special.

#### bluetooth-compose.yml

```
version: '0.1'

devices:
     breaths_per_minute:
        yardy-yardy: something
        etcetera:

     pulse_beats_per_minute:
        yardy-yardy: something
        etcetera:

logging:
        yardy-yardy: something
        etcetera:
```

We are a group of bluetooth-nutters who want to use yaml to not just automate the reading of bluetooth devices, but also to simulate devices to make integration testing easier.

### And for example, can you imagine:

- feeding a mobile app with a ```bluetooth-compose.yml``` file and have it automatically connect to a bluetooth device - with or without user-permissions, as required - to also auto-generate the associated UI - and then to forward those readings to another app, or to the cloud?

### Or what about:

- feeding a bluetooth peripheral IoT device with a ```bluetooth-compose.yml``` file and have it auto-configure it's advertisements, services, characteristics, and descriptors, so that a bluetooth central mobile app, or another bluetooth device, can connect to it!

*We can!*

As implied, these are the two ravens in the bluetooth-compose logo, bluetooth-compose as a common language, with one such bluetooth-compose.yml file being the voice of the ```ble central``` raven, and the other such bluetooth-compose.yml file being the voice of the ```ble peripheral``` raven ~ both speaking to the bluetooth king!

This is the new era of ```Bluetooth-Compose-On-Demand (BCOD)``` where an app or device can dynamically consume bluetooth-compose.yml files for dynamic bluetooth engagement where even IoT devices can dynamically adjust to the changing environment around them.

- ```Bluetooth-Compose-On-Demand (BCOD)``` effectively manifests as ```Bluetooth-Peripheral-On-Demand (BPOD)```, where the automation of the first effectively manifests as the second!

And if that's not powerful enough to consider, just imagine ```bluetooth-compose.yml``` files being passed around the bluetooth ether as ```bluetooth extended advertisements```! 

*Please excuse the poetic licence, but:*

- Bluetooth 'mists' of bluetooth-compose.yml files existing in the bluetooth radio frequency space all around you, which we describe as existing in the electronically conspicuous bluetooth advertisement atmosphere where they collectively consolidate to become associated as bluetooth droplets of combined units of ble central-and-peripheral bluetooth-compose associates, with those bluetooth droplets precipitating as bluetooth rain that falls upon the bluetooth escarpment of devices that are composed by such bluetooth-compose file-associations to turn into streamlets of bluetooth data-flows, and for those streamlets of bluetooth dataflows combining into brooks of bluetooth data flows that converge into rivers of bluetooth data flows, which all flood into and become the bluetooth ocean - with ravens flying across the waters of the deep until the dove-of-peace is released: 

*IoT + Bluetooth-Compose = IoT 'Electronic Conspicuity' ... 'EoT' for short!*

In otherwords, we are on the verge of IoT 1.0 graduating to a new IoT 2.0 version of itself called EoT, an 'Electronic Conspicuity of Things'.

## Introduction to Terminology

The following terms are used in this document and the reason for so many acronyms is that (in lower-case) they will be useful abbreviations that can become part of the actual bluetooth-compose.yml file syntax.

### Aide Memoire 

In general, letter sub-parts in acronyms **might** mean one of the following:

- A, App
- M, Module,
- F, file
- U, Unity or Unreal (graphic engines)
- AR, Augmented Reality
- BC, Bluetooth-Compose
- OD, On-Demand, or Object Detection
- ML, Machine Learning
- VR, Virtual Reality
- HUD, Head-Up Display

### Terminology

- .NET Maui
- AR App, ARA
- AR Module, ARM
- Augmented Reality, AR
- Augmented-Reality-Head-Up Display, ARHUD: a general Head-Up Display (HUD) in the Visual AR Space (VARS)
- BCAR ML File, BCARMLF
- Bluetooth Compose Augmented Reality, BCAR
- Bluetooth Compose Augmented Reality App, BCAR App, BCARA
- Bluetooth-Compose Augmented Reality Kotlin Multiplatform App, BCARKMP App, BCARKMPA
- Bluetooth-Compose Augmented Reality .NET Maui App, BCARDNM App, BCARDNMA
- Bluetooth-Compose AR Module, BCAR Module, BCARM
- Bluetooth-Compose Specification, BCS
- Bluetooth-Compose-On-Demand, BCOD
- Bluetooth-Compose, BC
- bluetooth-compose.yml, BCY
- Bluetooth-Head-Up Display, BHUD: a bluetooth's device Head-Up Display (HUD) in the Visual AR Space
- Bluetooth-Peripheral-On-Demand, BPOD
- Electronic Conspicuity, EC (contextual)
- Electronically Conspicuous, EC (contextual)
- Elspic, short for Electronic Conspicuity, Electronically Conspicuous (contextual)
- EoT
- GUID
- Head-Up Display, HUD
- IoT
- Kotlin Multiplatform
- Machine Learning On Demand, MLOD
- Machine Learning On Demand File, MLODF
- Machine Learning, ML
- Meadow F7v2 Board (Wilderness Labs)
- Minimal Viable Product, MVP
- MLOD File, MLODF
- nRF52 Series Board (Nordic)
- Object Detection, OD
- Radiate-and-Receive, RAR
- Repository ('repo')
- Swift
- Unity
- Unity AR Module, UAR Module, UARM
- Virtual Reality, VR
- Virtual-Reality-Head-Up Display, VRHUD: a general Head-Up Display (HUD) in the Visual VR Space
- Visual AR Space, VARS
- Wifi-Head-Up Display, BHUD: a wifi device Head-Up Display (HUD) in the Visual AR or VR Space
- Wonderwall
- Xamarin

## So let's first explain the importance of the Bluetooth-Compose logo!

When the Bluetooth name and logo were coined, over two decades ago, it was important to choose a name and logo that acted as a metaphor about how the new technology worked. 'Bluetooth', of course, was an ancient Norse king who united the surrounding tribes to work as a coordinated whole. And the logo is actually the ancient runic 'b' and 't' superimposed into a singularity. We take the logo for granted now, because we understand how it represents the technology.

However *now*, over two decades later, we need to extend the metaphor - how do we explain Bluetooth in a new context where a standardised 'bluetooth-compose' yaml will effectively be a new language that is a bridge between ble centrals and ble peripherals, but maintaining the evolving yet 'quintessential bluetooth' protocol in the middle?

We've gone back to the Norse god, Odin, who had a raven on each shoulder!

- Two ravens who talk to the king betwixt - that old Norse king, 'Bluetooth', with a raven upon one shoulder, and a raven upon the other shoulder, with communication between them perfectly composed!

*Bluetooth-Compose!*

## It's all about communication!

So we've registered the following domains (no websites yet!), and engaged a graphic artist to create the logo:

```
- bluetooth-compose.com
- bluetooth-compose.net
- bluetooth-compose.org
- bluetooth-compose.co.uk
```

(and ```bluetoothcompose.com``` just for package ids, if you use Android Studio, you'll know what this is all about!)

We're just getting started but you can email us at super.admin@bluetooth-compose.com if you are at all enthusistic about this!

### Where we began - the concept of a smart city being 'Electronically Conspicuous'

Although Bluetooth-Compose is an open-source project (but with trademarks presently retained), it has it's roots in a private 'smart-city' project called [Electronically Conspicuous](https://www.youtube.com/watch?v=0K6vrWz2AuQ), whose prototype was rapidly run out using 'Wifi Aware', but which inevitably graduated to requiring Bluetooth 5 *extended-advertising* capability so that any ordinary smart-phone could engage with any 'little-red-box' (LRB) that sat upon any one of a number network-connected street-light posts that were 183 meters apart, or less. 

One thing that originally spiked our interest is that the above-mentioned figure of 183 meters actually derives from the matter that in the United Kingdom, a 'built-up area' is defined in legislation as being (paraphrased) 'where street-light-posts are 183 meters apart or less'.

- As it turns out, there is a commercial relationship between population density and the obligation of local authorities to provide street-lighting that meets a minimum distance-based specification - and that means that there is already the obligation of local authorities to provide street-light-posts as essentially being already available to be used as 'defacto-smart-city-transmission-towers' and which defacto smart-city-transmission-tower density is thus already budgetted to increase as population density increaeses.
- The knock-on effect of this - and which is the focus of the private venture - is that 'smart-cities' can be defined as places where smart-city radio-broadcast transmission-towers are placed 183 meters apart, or less, with the implication being that each street light post is a defacto 'transmission-tower' upon which a smart-city 'little-red-box' can be placed to be accessible to any mobile or static device that has Bluetooth radio-transmission  capability.

*Why is such a smart-city radio-communications node referred to as 'a Little Red Box'?*

- Quite simply, the colour red symbolises 'radiate and receive', which is a metaphor for the quintessential client-server relationship but expresses that the underlying transport layer is the medium or radio-communication.
 
**The expression 'Radiate-and-Receive' (RAR) = a 'Radio-Communications based Client-Server relationship'**

Where Bluetooth-Compose comes into play as part of the original patent-pending abstraction, is that a central hub that feeds all of the 'street-light-post LRBs' would need to feed each LRB with an auto-configuring text-stream - each tailored to each one's culturo-geographic context - and the result of that text-stream would be that each LRB device would be capable of broadcasting culturo-geographic (i.e. location-tailored) information that is based upon smart-phone accessible bluetooth-services that magically autoconfigured themselves according to the structure of the bluetooth-compose  text-stream; the advantage of this, of course, is that Bluetooth 5 would also allow each device to retrieve it's own bluetooth-compose file from each repective LRB  so that it could set up it's own symbiotic relationship with that LRB that was preconfigured by the original text-stream, to broadcast the presence of a specified peripherl.

- The correlation of the 183 meters or less spacing of the smart-city LRBs is that the Bluetooth specification is capable of scaling up to meet smart-city demands without becoming over-saturated.

Two concepts arose from this, 'bluetooth-peripheral-on-demand', and bluetooth-compose-on-demand, with the latter being the intrinsic cause of the former to be capable of manifesting.

Yet despite being a private project - with a patent application pending - it was realised that inevitably, for the Electronically Conspicuous service to be consumed en-mass, the bluetooth-compose specification would have to develop as open-source, with the advantage being that whereas it could still be consumed by the original private context (and in doing so, maintaining it's own patent-pending commercial significances), that bluetooth-compose could also be consumed in a myriad of circumstances that were unconnected with the original technology.

- An example of how bluetooth-compose can be used as totally unconnected with the original smart-city concept, is that the bluetooth-compose specification could also be used to auto-configure a simulated bluetooth peripheral, as part of an isolated overnight intergration-test build in a CI/CD pipeline. 

As such, the original concept of Electronic Conspicuity has expanded from it's broad smart-city concept to merely be the next evoltion of IoT as a universally ubiquitous "EoT" (i.e. electronically conspicuous IoT).

### Where we're going - as directed by an Open-Source Steering Committee

First of all, here's a shout-out to Chris Davies at [Velvet Skies](https://velvetskies.com/), who designed the Bluetooth-Compose logo. Chris was the UI head at Nokia Entertainment Worldwide, back in Nokia's glory-days, and given his demonstrated value-add to such a prominent global blue-chip, he is naturally this project's first port of call when it comes to UI and UX. The founder of this project worked with Chris at Nokia's Bristol office.

We are also seeking similarly qualified people to be part of the Bluetooth-Compose Steering Committee, the minimum job is to merely keep an eye on what's going on and feed back the occasional comment, and perhaps also attending the occasional steering committee Teams meeting (or something of that ilk).

Of course, you will be able to fill your boots and do as much as you want, but we will need qualified eyes that at a minumum will assist us to track onwards with integrity so that we can constantly and consitently, nudge the leading edge forwards.

If you are qualifed by some nuance to be on the Steering Committee, please either contact us using the above email address, or - if you already know someone who is involved - through a private channel that they'd recommend.

Members of the steering committee will be 'ravens flying back and forth over the waters of the deep', and as we know from history, 'ravens always get from one side of floods of adversity, to the other', doing so, so that 'the dove of peace can be released'.

*The question that the team has before it, is 'What does the dove of peace look like'?*

- Will it be IoT 1.0 graduationg to become IoT 2.0? 
- Will it be Web 3.0 maturing to become Web 4.0 by everything becoming Electronically Conspicuous?

*Those who fly back and forth over the waters of the deep will be the first to know!*

### Our Initial Aim

Our initial aim is to create an alpha ```bluetooth-compose.yml``` specification as a candidate for community discussion, to move onwards towards beta.

And there's a lot to discuss, not least of which is maintaining a separation of concerns between automatically generated UI and the underlying automatically generated advertisements, peripherals, services, characteristics, and descriptors - not to forget the complexity of how to connect to such devices, engage with user-permissions, etc.

And a full-blown discussion must also include the adjacent but not unconnected topic of ```Augmented Reality```, where not only are devices pictorially identified in the ```visual AR-space``` - with or without visual information that a user can read in a ```'Bluetooth-Head-Up Display' (BHUD)``` context similar to fighter-jet Head-Up Display (HUD) - but also incorporating the capacity of Bluetooth Device Object Detection that allows a user to visually scan a device, which once recognised using the Machine Learning mechanism of Object Detection - and typically through a user gesture that grants user-permission - will cause the app to autoconfigure to take a reading.

- An easy inroad into realising why Bluetooth Devices and Augmented Reality will become so instrinsically intertwined, is that the mechanism of a device's 'Bluetooth Head Up Display' in the AR space is essentially just that device's instrumentation that we would ordinarily expect from it in the Real-world space, but projected as a Head up Display in the Augmented Reality space. And a device's instrumentation is just the start - any information that could be associated with a device could be pulled up on demand, from APIs through specifications to tutorials, to the display of real-time data. Curiously, the mechanic of moving the real-world instrumentations of devices into the Augmented Reality space, will significantly reduce that carbon footprint that is typically associated with the cost of creating real-world device instrumentation. *Now there's a bonus to pacify the greenies!*

This wondrous new space we are in is imagining an environment that has plenty of bluetooth devices in it, each which are [electronically conspicuous](https://github.com/HarrisonOfTheNorth/Elspic-IoT-the-next-generation) to an ```Augmented Reality application```, and dependant upon user-engagement with the device in that AR-space, the dynamic auto-configuring of channels between the ```AR app``` and ```BCOD``` device so that they can be read, or written to, on demand.

*Bluetooth-Compose-On-Demand, BCOD.*

#### But first-things-first

Naturally, the specification will be all about how to generate the yml files however as an interoperability proof we will wish to demonstrate their consumption in the following frameworks, hence will create demo bluetooth-compose engines for the following frameworks:

```
- Xamarin / .Net Maui (DNM)
- Kotlin Multi Platform (KMP)
- Swift (SW)
```

And for simulating devices for Integration Testing, we are focussing on two device series:

```
-    C# Meadow F7v2 boards, using the Meadow API at http://developer.wildernesslabs.co/Meadow/Meadow.OS/Bluetooth/
-    C++ Nordic's nRF52 series at https://www.nordicsemi.com/Products/Bluetooth-Low-Energy/Development-hardware
```

Please notice the synchronicity between Unity Engine AR and the Meadow F7v2 boards being written in C#, and the Unreal Engine AR and Nordic's nRF52 series being written in C++.

*This is not to say that polyglots can't mix and match both C# and C++ in the same way that they already mix these with Kotlin, Swift, and Xamarin / .NET Maui, but is just to say that small team early adopters will be able to hit the ground running!*

## Current Status

To consume these peripherals (the [C# Meadow F7v2 boards](http://developer.wildernesslabs.co/Meadow/Meadow_Basics/Hardware/F7v2/) and [C++ Nordic's nRF52 series](https://www.nordicsemi.com/Products/Bluetooth-Low-Energy/Development-hardware) devices)  we will simultaneously create an Augmented Reality app (initially, just in Kotlin Multiplatform, using Unity AR, but with the Unity library exportable to Xamarin, .Net Maui, and Swift). We would hope that a similar Unreal version would follow soon after.

The Bluetooth AR app and it's associated architecture (that is, it's underlying Unity AR module/s that will be capable of being exported to any suitable platform), will also need to be capable of importing relevant ML (machine learning profiles) so that specific devices can be detected in the user's local AR space.

To make this project exciting, but also giving it the opportunity to go viral, this project will be Augmented-Reality led, and that means that our incremental advances in the bluetooth-compose.yml specification must be capable of being demonstrated in our Bluetooth Compose Augmented Reality app.

The focus, however, will be the specification itself, and we would be delighted that as the specification matures, that third parties will create their own bluetooth-compose engines, as well as Augmented Reality applications, on any platform that they desire.

The current status of the specifications can be determined by the status of the progression of the following milestones.

### MVP Milestones:

The Current Milestone being Undertaken: **Kotlin Multiplatform Consumption of a Separate Unity Module**

See the [KMP app](https://github.com/Bluetooth-Compose/BCARKMPApp) and [Unity AR Module](https://github.com/Bluetooth-Compose/BCARM-Unity) repos for specific progress

#### Milestones

##### Phase 1 Milestones

1. Kotlin Multiplatform Consumption of a Separate Unity AR Module
1. The Creation of a Machine Learning Object-detectable 3d Printed Bluetooth Device Container
1. The Inbuilt Capability of the Ar Module to Detect the 3d Printed Bluetooth Device Container
1. The Creation and Consumption of a Preliminary bluetooth-compose.yml File With Object Detection Capability
1. The Creation of an Additional 3d Printed Bluetooth Device Container
1. The Preliminary Interaction of the Bcar App With Two Bluetooth Devices
1. The Capability to Read Bluetooth Service Guids in Ar Space
1. The Capability to Read Bluetooth Service, Characteristic, and Descriptor GUIDS, in AR space
1. Community Discussion Event
1. The Creation of Phase 2 Milestones

#### Details

1. KOTLIN MULTIPLATFORM CONSUMPTION OF A SEPARATE UNITY AR MODULE<br />The creation of a Unity AR 'hello-world' module in it's own repository that is consumed by a Multiplatform hello-world world application that is separate in it's own repository but which loads that Unity AR module, which from here are collectively called the Bluetooth-Compose AR App ('BCAR app'), and which is capable of deploying to iOS and Android devices, with the initial intention being to ultimately deploy to the Google Play Store, and Apple App Store. 
     1. The creation of the Multiplatform [application](https://github.com/Bluetooth-Compose/BCARKMPApp) (as consumes the above [Unity AR module](https://github.com/Bluetooth-Compose/BCARM-Unity)), in Kotlin Multiplatform, outputing Android and iOS applications
     2. The creation of the Multiplatform application (as consumes the above Unity module), in .NET Maui, outputing Android and iOS applications
     3. The versions that will ultimately be deployed to the Google Play Store, and Apple App Store, with be the Kotlin Multiplatform versions.
     4. Acceptance Criteria: The functionality in the iOS and Android deployment of the stand alone Unity App will appear via git submodule deployment to the KMP Repo and upon build and deployment will be visible in both iOS and Android deployments of the KMP app.
3. THE CREATION OF A MACHINE LEARNING OBJECT-DETECTABLE 3D PRINTED BLUETOOTH DEVICE CONTAINER<br />The creation of a 3D Printed bluetooth device container that will ultimately be identified by the app's AR-space object detection mechanism via a user-loaded bluetooth-compose.yml file pointing to an external Machine Learning (ML) object detection file that will allow the BCAR App to identify it.
     1. For simplicity, the first series of Bluetooth Device Object Detection containers will consist of common container shapes however the bluetooth devices in those containers will be differentiated by unique printed cards that are attached to the container tops.
     1. In this way, the Object Detection Machine Learning will seek to identify the existence of such cards, and when such cards are identified in the AR space, a Bluetooth Head Up Display (BHUD) will be associated with such cards in much the same way that we see object detected cards that depict cartoon characters, suddenly come to life with 3D models dancing upon the detected cards. 
     1. This milestone consists of merely creating the actual device container, with a suitable device-card actually attached to the container's top, so that it is suitable of being the subject of the future AR Object Detection milestones.
     1. Acceptance Criteria: The container must be capable of holding either of a Meadow F7V2 board, or a nRF52 Series board that is yet to be identified; the card that that is attached to the top of the container must be at least the size of a playing card.
4. THE INBUILT CAPABILITY OF THE AR MODULE TO DETECT THE 3D PRINTED BLUETOOTH DEVICE CONTAINER<br />The built-in capability of the BCAR app to identify this specific 3D Printed Bluetooth Device Container.
     1. The built-in object detection method must be capable of being dynamically loaded into the Unity Module in the following milestone.  
6. THE CREATION AND CONSUMPTION OF A PRELIMINARY BLUETOOTH-COMPOSE.YML FILE WITH OBJECT DETECTION CAPABILITY<br />The creation of a bluetooth-compose.yml specification that the BCAR App can load and consume under manual user-selection, which bluetooth-compose.yml file will identify a web-based ML file that the AR module seeks, consumes, then uses, to identify that particular 3D Printed Bluetooth Device Container.
     1.  Such a web based ML file will be stored in a separate Bluetooth-Compose repo (the 'BCAR ML file repo') that can be a hub for all future public BCAR ML files that are capable of being consumed by the application.
     2.  Acceptance Criteria: The bluetooth-compose.yml must be specified, a toolkit must be created that allows the ML file to be created and must include end-to-end instructions from creating the initial photographs of a bluetooth device in question, through training a model, to how bluetooth-compose.yml file would allow the ML output to be consumed from the web so that it can be the basis of object detection in the BCAR App.
8. THE CREATION OF AN ADDITIONAL 3D PRINTED BLUETOOTH DEVICE CONTAINER<br />The creation of a second 3D Printed Bluetooth Device Container, with instructions on how to create further device ML files, allowing community developers to both create them and upload them to the common BCAR ML file repo. The instructions should identify where the associated 3D Printing files are located, if pertinent.
9. THE PRELIMINARY INTERACTION OF THE BCAR APP WITH TWO BLUETOOTH DEVICES<br />The extension of the bluetooth-compose.yml specification to allow demonstration Meadowboard and/or nRF series bluetooth devices to be placed in respective 3D Printed Bluetooth Device Containers, which when identified in AR-space by the BCAR App, will cause the bluetooth device's advertised name to be displayed within the BCAR App's visual AR-space, adjacent the identified device.
10. THE CAPABILITY TO READ BLUETOOTH SERVICE GUIDS IN AR SPACE<br />The extension of the bluetooth-compose.yml specification to allow a user tapping on such a Bluetooth Device that has been so identified in AR-space, to connect to the device and read it's service GUIDs and display them in a Console.
10. THE CAPABILITY TO READ BLUETOOTH SERVICE, CHARACTERISTIC, & DESCRIPTOR GUIDS IN AR SPACE<br />The extension of the bluetooth-compose.yml specification to allow a user tapping on such a Bluetooth Device that has been so identified in AR-space, to connect to the device and read it's service GUIDs, Characteristic GUIDS, and Descriptor GUIDS, and display them in a Console. 
     1. The bluetooth-compose.yml specification that does this should be test-context related, and be a simple yml command of the ilk 'dump-guids' and which merely cascades through the guids that it can acquire without having to engage in any security mechanism: the output is to be food for thought for the community discussion event milestone.
12. COMMUNITY DISCUSSION EVENT<br />The creation of a community event to allow the bluetooth-compose specification, as developed so far, to be demonstrated, discussed in a retrospective context, and a wide-range of use cases gathered from the community to steer the project forward.
13. THE CREATION OF PHASE 2 MILESTONES<br />The creation of new milestones, based upon feedback from the community event.

Naturally, once we get to this stage, users will be able to create AR ML files for any Bluetooth device on the market, enabling them to be intelligently object detected in the Bluetooth Compose AR Module, using such Machine Learning profiles as developers will create, and at this point we would hope that the project would begin to go viral as developers start to create AR ML files in anticipation of the Bluetooth-Compose specification being extended further to allow the devices to not just be identified in the local AR-space, but then read, and written to, according to the **device's** own capability.

- We would also hope that from this moment, that whilst various parts of the Bluetooth-Compose community continue to develop the next part of the bluetooth-compse.yml specification, that other parts of the Bluetooth-Compose community will be engaged in creating a vast library of bluetooth-compose.yml files that will allow almost any bluetooth device that is on the market, to be identified in AR space. 
- Of course, during this rapid expansion of bluetooth device object detection capability in the AR space, that the most that any device-specific bluetooth-compose.yml file will be able to do is to both enable a device to be object-detected in AR space, as well as having it's guids dumped to a console ... but givenm this, the next phase will be able to categorise these devices into complexity groups with the hope that by the end of the phase that some bluetooth devices will be capable of being 'fully-composed' (i.e. 'capable of full interaction as they were intended'), in any application that would want to consume the BCAR Module.

We recently [asked](https://www.youtube.com/watch?v=kzNXPFguYik&t=2701s) what IoT 2.0 will look like, and although we have some [idea](https://github.com/HarrisonOfTheNorth/Elspic-IoT-the-next-generation) of what this might look like, we surmise that this project will make a significant contribution!

### Further Reading

Given that Bluetooth-Compose in the context of Augmented Reality can be very easily seen to be 'Electronically Conspicuous' in the very real sense of the expression, you may be interested in looking at our thoughts [here](https://github.com/HarrisonOfTheNorth/Elspic-IoT-the-next-generation) that consider how ```IoT```, via the abstraction of *Electronic Conspicuity*, might transform ```IoT``` into becoming ```EoT```. 

- In that article, we propose that Electronic Conspicuity is IoT's wonderwall, and that Electronic Conspicuity will transform IoT into the next evolution of itself, EoT. 
